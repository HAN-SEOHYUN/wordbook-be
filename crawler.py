from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import re
import logging
import requests

# config import
import config

# ë¡œê¹… ì„¤ì • (configì—ì„œ ì½ê¸°)
logging.basicConfig(
    level=getattr(logging, config.LOG_LEVEL),
    format=config.LOG_FORMAT,
    datefmt=config.LOG_DATE_FORMAT,
)
logger = logging.getLogger(__name__)


class EBSMorningCrawler:
    """EBS ëª¨ë‹ìŠ¤í˜ì…œ ì˜ë‹¨ì–´ í¬ë¡¤ëŸ¬"""

    # ì •ê·œí‘œí˜„ì‹ íŒ¨í„´
    WORD_PATTERN = re.compile(r"â–¶\s*(.*?)\s*:\s*(.*?)(?=\s*â–¶|\Z)", re.DOTALL)
    NON_BULLET_WORD_PATTERN = re.compile(
        r"([a-zA-Z\s\-\/]+?):\s*(.*?)(?=\s*â–¶|\s*[a-zA-Z\s\-\/]+?:|\Z)", re.DOTALL
    )

    def __init__(self):
        """í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” (configì—ì„œ ì„¤ì • ì½ê¸°)"""
        chrome_options = Options()

        if config.HEADLESS_MODE:
            chrome_options.add_argument("--headless")

        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--disable-gpu")
        chrome_options.add_argument("--window-size=1920,1080")
        chrome_options.add_argument(
            "user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        )

        # ChromeDriver ì„¤ì¹˜ ë° ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
        driver_path = ChromeDriverManager().install()
        logger.info(f"ChromeDriver path: {driver_path}")

        # ê²½ë¡œê°€ THIRD_PARTY_NOTICES íŒŒì¼ì„ ê°€ë¦¬í‚¤ëŠ” ê²½ìš° ìˆ˜ì •
        if "THIRD_PARTY_NOTICES" in driver_path:
            import os
            driver_dir = os.path.dirname(driver_path)
            driver_path = os.path.join(driver_dir, "chromedriver.exe")
            logger.info(f"Fixed ChromeDriver path: {driver_path}")

        service = Service(driver_path)
        self.driver = webdriver.Chrome(service=service, options=chrome_options)
        self.wait = WebDriverWait(self.driver, config.PAGE_LOAD_TIMEOUT)

    def calculate_target_date(self, days_ago: int):
        """
        ë‚ ì§œ ê³„ì‚° í•¨ìˆ˜

        Args:
            days_ago: ë©°ì¹  ì „

        Returns:
            tuple: (í‘œì‹œìš© ë‚ ì§œ, DBìš© ë‚ ì§œ)
        """
        target_date = datetime.now() - timedelta(days=days_ago)
        display_format = target_date.strftime("%Y.%m.%d")
        db_format = target_date.strftime("%Y-%m-%d")
        return display_format, db_format

    def find_article_by_date(self, target_date: str):
        """ê²Œì‹œíŒì—ì„œ íŠ¹ì • ë‚ ì§œì˜ ê²Œì‹œê¸€ ì°¾ê¸°"""
        try:
            logger.info(f"ê²Œì‹œíŒ í˜ì´ì§€ ì ‘ì† ì¤‘...")
            self.driver.get(config.BOARD_URL)

            self.wait.until(
                EC.presence_of_all_elements_located((By.CSS_SELECTOR, "#itemList > tr"))
            )
            logger.info("í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ")

            rows = self.driver.find_elements(
                By.CSS_SELECTOR, "#itemList > tr:not(.notice)"
            )
            logger.info(f"ì´ {len(rows)}ê°œì˜ ì¼ë°˜ ê²Œì‹œê¸€ ë°œê²¬")

            for idx, row in enumerate(rows, 1):
                try:
                    tds = row.find_elements(By.TAG_NAME, "td")

                    if len(tds) < 5:
                        continue

                    # ê²Œì‹œê¸€ ì œëª©ì„ ê°€ì ¸ì˜µë‹ˆë‹¤. (ë°©ì†¡ì¼ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆì„ ê°€ëŠ¥ì„±ì´ ë†’ìŒ)
                    link_element = tds[1].find_element(By.TAG_NAME, "a")
                    title = link_element.text.strip()

                    # ê²Œì‹œíŒ ëª©ë¡ì˜ ë‚ ì§œë„ ê°€ì ¸ì˜µë‹ˆë‹¤. (ë””ë²„ê¹…/ì •ë³´ìš©)
                    date_text = tds[3].text.strip()

                    # â­ í•µì‹¬ ìˆ˜ì •: ê²Œì‹œíŒ ë‚ ì§œ ëŒ€ì‹ , ê²Œì‹œê¸€ ì œëª©ì— 'target_date'ê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
                    # target_dateëŠ” 'YYYY.MM.DD' í˜•ì‹ì´ê³ , titleì€ 'YYYY.MM.DD. Day. (...)' í˜•ì‹ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                    if target_date in title:

                        href = link_element.get_attribute("href")

                        if href.startswith("/"):
                            full_url = config.BASE_URL + href
                        else:
                            full_url = href

                        logger.info(f"âœ“ ë°œê²¬: {title} (ê²Œì‹œì¼: {date_text})")
                        logger.info(f"  URL: {full_url}")

                        return full_url

                except Exception as e:
                    logger.debug(f"ê²Œì‹œê¸€ {idx} ì²˜ë¦¬ ì¤‘ ì—ëŸ¬: {e}")
                    continue

            logger.warning(f"{target_date} ë‚ ì§œì˜ ê²Œì‹œê¸€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None

        except Exception as e:
            logger.error(f"ê²Œì‹œíŒ ì¡°íšŒ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
            return None

    def clean_korean_translation(
        self, raw_kor: str, log_prefix: str, final_matches: list
    ) -> str:
        """í•œê¸€ í•´ì„ í´ë¦¬ë‹"""
        clean_kor = raw_kor

        relieve_pattern = r"relieve oneself\[nature\]\s*:\s*ëŒ€ì†Œë³€ì„ ë³´ë‹¤"
        relieve_match = re.search(relieve_pattern, clean_kor, re.DOTALL | re.IGNORECASE)

        if relieve_match:
            relieve_entry_text = relieve_match.group(0).strip()
            relieve_parts = relieve_entry_text.split(":")

            if len(relieve_parts) == 2:
                relieve_eng = relieve_parts[0].strip().replace("[nature]", "").strip()
                relieve_kor = relieve_parts[1].strip()
                final_matches.append((relieve_eng, relieve_kor))
                logger.debug(f"{log_prefix} [Clean] Extracted 'relieve oneself'.")

            cut_index = relieve_match.start()
            clean_kor = clean_kor[:cut_index].strip()

        news_body_start_match = re.search(
            r"\s*\d+\.\s*([A-Z]|\s*ì„œìš¸ì‹œëŠ”|\s*ì˜êµ­|\s*ë¯¸êµ­ì˜)", clean_kor, re.DOTALL
        )

        if news_body_start_match:
            cut_index = news_body_start_match.start()
            clean_kor = clean_kor[:cut_index].strip()
            logger.debug(f"{log_prefix} [Clean] Cut at News Item Start")

        expression_marker = "Expression"
        if expression_marker in clean_kor:
            cut_index = clean_kor.find(expression_marker)
            clean_kor = clean_kor[:cut_index].strip()
            logger.debug(f"{log_prefix} [Clean] Cut at Expression Marker")

        return clean_kor.strip()

    def extract_vocabulary(self, article_url):
        """ê²Œì‹œê¸€ì—ì„œ ì˜ë‹¨ì–´ ì¶”ì¶œ"""
        try:
            logger.info("ê²Œì‹œê¸€ ìƒì„¸ í˜ì´ì§€ ì ‘ì† ì¤‘...")

            response = requests.get(article_url, timeout=config.HTTP_TIMEOUT)
            response.raise_for_status()

            soup = BeautifulSoup(response.text, "lxml")
            body_container = soup.select_one(config.BODY_SELECTOR)

            if not body_container:
                logger.error(
                    f"Selector '{config.BODY_SELECTOR}'ë¥¼ ì‚¬ìš©í•˜ì—¬ ë³¸ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                )
                return []

            raw_text = " ".join(body_container.stripped_strings)
            full_text = re.sub(r"\s+", " ", raw_text).strip()

            logger.info(f"ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ (ê¸¸ì´: {len(full_text)}ì)")

            parts = full_text.split("Expression ]")
            vocabulary_text = " ".join([part.strip() for part in parts[1:]])

            vocabulary_text = vocabulary_text.split("idiom package")[0].strip()
            vocabulary_text = re.split(r"-{10,}", vocabulary_text)[0].strip()
            vocabulary_text = vocabulary_text.split(
                "NEWS COVERAGE FROM THE NEW YORK TIMES"
            )[0].strip()

            final_matches = []

            matches_bullet = self.WORD_PATTERN.findall(vocabulary_text)

            for i, (eng, raw_kor) in enumerate(matches_bullet):
                log_prefix = f"[ë‹¨ì–´ {i+1:02d}] '{eng.strip()[:20]}...'"
                clean_kor = self.clean_korean_translation(
                    raw_kor, log_prefix, final_matches
                )
                final_matches.append((eng.strip(), clean_kor))

            matches_non_bullet = self.NON_BULLET_WORD_PATTERN.findall(vocabulary_text)
            bullet_engs = {m[0] for m in final_matches}

            for eng, raw_kor in matches_non_bullet:
                if eng.strip() not in bullet_engs and len(eng.strip()) > 2:
                    clean_kor = raw_kor.strip().split("Expression")[0].strip()
                    clean_kor = re.split(r"\d+\.\s*", clean_kor)[0].strip()

                    if not clean_kor.startswith(eng):
                        final_matches.append((eng.strip(), clean_kor))

            logger.info(f"âœ“ ì´ {len(final_matches)}ê°œì˜ ë‹¨ì–´ ì¶”ì¶œ ì™„ë£Œ")

            for idx, (eng, kor) in enumerate(final_matches, 1):
                logger.info(f"  [{idx}] {eng} : {kor}")

            words = [
                {"english_word": eng, "korean_meaning": kor}
                for eng, kor in final_matches
            ]

            return words

        except Exception as e:
            logger.error(f"ì˜ë‹¨ì–´ ì¶”ì¶œ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
            import traceback

            traceback.print_exc()
            return []

    def save_to_database(self, date, words, source_url):
        """DB ì €ì¥ (config.DIRECT_DB_SAVEì— ë”°ë¼ API ë˜ëŠ” ì§ì ‘ ì €ì¥)"""

        if config.DIRECT_DB_SAVE:
            self._save_to_db_directly(date, words, source_url)
        else:
            self._save_via_api(date, words, source_url)

    def _save_via_api(self, date, words, source_url):
        """APIë¥¼ í†µí•´ ì €ì¥"""

        success_count = 0
        fail_count = 0

        logger.info(f"APIë¥¼ í†µí•´ ì €ì¥ ì¤‘...")

        for word in words:
            try:
                data = {
                    "date": date,
                    "english_word": word["english_word"],
                    "korean_meaning": word["korean_meaning"],
                    "source_url": source_url,
                }

                response = requests.post(
                    config.API_ENDPOINT, json=data, timeout=config.HTTP_TIMEOUT
                )

                if response.status_code in [200, 201]:
                    success_count += 1
                else:
                    fail_count += 1
                    logger.warning(f"  âœ— ì €ì¥ ì‹¤íŒ¨: {word['english_word']}")

            except Exception as e:
                fail_count += 1
                logger.error(f"  âœ— ì €ì¥ ì¤‘ ì—ëŸ¬: {word['english_word']} - {e}")

        logger.info(f"ì €ì¥ ì™„ë£Œ: ì„±ê³µ {success_count}ê°œ, ì‹¤íŒ¨ {fail_count}ê°œ")

    def _save_to_db_directly(self, date, words, source_url):
        """DBì— ì§ì ‘ ì €ì¥"""
        from core.database import DatabaseManager

        logger.info(f"DBì— ì§ì ‘ ì €ì¥ ì¤‘...")

        db_manager = DatabaseManager()

        upsert_query = """
        INSERT INTO daily_vocabulary (date, english_word, korean_meaning, source_url)
        VALUES (%s, %s, %s, %s)
        ON DUPLICATE KEY UPDATE
            korean_meaning = VALUES(korean_meaning),
            source_url = VALUES(source_url),
            updated_at = CURRENT_TIMESTAMP;
        """

        success_count = 0
        fail_count = 0

        try:
            with db_manager.get_connection() as conn:
                with conn.cursor() as cursor:
                    for word in words:
                        try:
                            cursor.execute(
                                upsert_query,
                                (date, word["english_word"], word["korean_meaning"], source_url),
                            )
                            success_count += 1
                        except Exception as e:
                            fail_count += 1
                            logger.error(f"  âœ— ì €ì¥ ì‹¤íŒ¨: {word['english_word']} - {e}")

                    conn.commit()

            logger.info(f"ì €ì¥ ì™„ë£Œ: ì„±ê³µ {success_count}ê°œ, ì‹¤íŒ¨ {fail_count}ê°œ")

        except Exception as e:
            logger.error(f"DB ì €ì¥ ì¤‘ ì—ëŸ¬: {e}")

    def run(self):
        """í¬ë¡¤ëŸ¬ ì‹¤í–‰"""
        try:
            logger.info("=" * 60)
            logger.info("EBS ëª¨ë‹ìŠ¤í˜ì…œ ì˜ë‹¨ì–´ í¬ë¡¤ëŸ¬ ì‹œì‘")
            logger.info("=" * 60)

            # ì„¤ì • ì¶œë ¥
            config.print_config()

            # ë‚ ì§œ ê³„ì‚°
            display_date, db_date = self.calculate_target_date(config.DAYS_AGO)
            logger.info(f"ëŒ€ìƒ ë‚ ì§œ: {display_date} (DB: {db_date})")

            # ì¬ì‹œë„ ë¡œì§
            if config.AUTO_RETRY_PREVIOUS_DATE:
                logger.info(
                    f"ìë™ ì¬ì‹œë„ í™œì„±í™”: ìµœëŒ€ {config.MAX_RETRY_DAYS}ì¼ ì „ê¹Œì§€ ì‹œë„"
                )
                date_range = []
                for i in range(
                    config.DAYS_AGO, config.DAYS_AGO + config.MAX_RETRY_DAYS
                ):
                    date_range.append(self.calculate_target_date(i))
            else:
                date_range = [(display_date, db_date)]

            article_url = None
            selected_db_date = None

            # ë‚ ì§œë³„ ê²Œì‹œê¸€ ê²€ìƒ‰
            for display, db in date_range:
                logger.info(f"ğŸ“… ë‚ ì§œ ì‹œë„: {display}")
                article_url = self.find_article_by_date(display)

                if article_url:
                    selected_db_date = db
                    break

            if not article_url:
                logger.warning("í¬ë¡¤ë§í•  ê²Œì‹œê¸€ì´ ì—†ìŠµë‹ˆë‹¤.")
                return

            # ì˜ë‹¨ì–´ ì¶”ì¶œ
            words = self.extract_vocabulary(article_url)

            if not words:
                logger.warning("ì¶”ì¶œëœ ë‹¨ì–´ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return

            # DB ì €ì¥
            logger.info(f"ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹œì‘ (ë‚ ì§œ: {selected_db_date})")
            self.save_to_database(selected_db_date, words, article_url)

            logger.info("=" * 60)
            logger.info("âœ… í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì™„ë£Œ!")
            logger.info("=" * 60)

        except Exception as e:
            logger.error(f"í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì¤‘ ì—ëŸ¬: {e}", exc_info=True)
        finally:
            self.close()

    def close(self):
        """ë¸Œë¼ìš°ì € ì¢…ë£Œ"""
        if self.driver:
            self.driver.quit()
            logger.info("ë¸Œë¼ìš°ì € ì¢…ë£Œ")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    # ì„¤ì • ê²€ì¦
    try:
        config.validate_config()
    except ValueError as e:
        logger.error(f"ì„¤ì • ì˜¤ë¥˜: {e}")
        return

    crawler = EBSMorningCrawler()
    crawler.run()


if __name__ == "__main__":
    main()
